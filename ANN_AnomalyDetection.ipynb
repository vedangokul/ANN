{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e11d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11162, 52)\n",
      "(8929, 51)\n",
      "(2233, 51)\n",
      "(8929, 33)\n",
      "(2233, 33)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.utils.data import generate_data\n",
    "\n",
    "df = pd.read_csv('bank.csv')\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n",
    "\n",
    "LE=LabelEncoder()\n",
    "df['deposit']=LE.fit_transform(df.deposit.values)\n",
    "\n",
    "#encoding categorical features\n",
    "df=pd.get_dummies(df)\n",
    "print(df.shape)\n",
    "\n",
    "#partitioning\n",
    "for train_index, test_index in sss.split(df.drop(\"deposit\",axis=1), df.deposit):\n",
    "    traindf=df.loc[train_index]\n",
    "    testdf= df.loc[test_index]\n",
    "\n",
    "#partition x/y\n",
    "xtrain=traindf.drop('deposit', axis=1)\n",
    "ytrain=traindf.deposit\n",
    "\n",
    "xtest=testdf.drop('deposit', axis=1)\n",
    "ytest=testdf.deposit\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "xtest = scaler.fit_transform(xtest)\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "\n",
    "pca = PCA(0.9)\n",
    "xtrain = pca.fit_transform(xtrain)\n",
    "xtest = pca.fit_transform(xtest)\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1f82bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 68        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 33)                99        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 33)                1122      \n",
      "=================================================================\n",
      "Total params: 4,661\n",
      "Trainable params: 4,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 2.5908 - val_loss: 1.5670\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.3289 - val_loss: 1.2016\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.1534 - val_loss: 1.1256\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.1091 - val_loss: 1.0974\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0878 - val_loss: 1.0811\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0743 - val_loss: 1.0697\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0643 - val_loss: 1.0609\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0561 - val_loss: 1.0540\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0497 - val_loss: 1.0483\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0445 - val_loss: 1.0435\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0400 - val_loss: 1.0393\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0357 - val_loss: 1.0357\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0325 - val_loss: 1.0326\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0293 - val_loss: 1.0298\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0266 - val_loss: 1.0273\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0241 - val_loss: 1.0251\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0219 - val_loss: 1.0232\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0201 - val_loss: 1.0214\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0183 - val_loss: 1.0198\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0168 - val_loss: 1.0183\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0153 - val_loss: 1.0170\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0140 - val_loss: 1.0157\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0128 - val_loss: 1.0146\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0117 - val_loss: 1.0136\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0107 - val_loss: 1.0126\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0097 - val_loss: 1.0118\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0089 - val_loss: 1.0110\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0081 - val_loss: 1.0103\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0074 - val_loss: 1.0096\n",
      "Epoch 30/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0068 - val_loss: 1.0090\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0062 - val_loss: 1.0084\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0056 - val_loss: 1.0079\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0051 - val_loss: 1.0074\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0047 - val_loss: 1.0070\n",
      "Epoch 35/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0043 - val_loss: 1.0066\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0039 - val_loss: 1.0062\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0035 - val_loss: 1.0060\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0032 - val_loss: 1.0056\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.0029 - val_loss: 1.0053\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0027 - val_loss: 1.0051\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0024 - val_loss: 1.0049\n",
      "Epoch 42/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0022 - val_loss: 1.0047\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0020 - val_loss: 1.0045\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 1.0043\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0016 - val_loss: 1.0042\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0015 - val_loss: 1.0040\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0014 - val_loss: 1.0038\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0013 - val_loss: 1.0037\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0011 - val_loss: 1.0036\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0010 - val_loss: 1.0035\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0009 - val_loss: 1.0034\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0008 - val_loss: 1.0034\n",
      "Epoch 53/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0008 - val_loss: 1.0033\n",
      "Epoch 54/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0007 - val_loss: 1.0032\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0006 - val_loss: 1.0031\n",
      "Epoch 56/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0006 - val_loss: 1.0031\n",
      "Epoch 57/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0005 - val_loss: 1.0030\n",
      "Epoch 58/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0005 - val_loss: 1.0030\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0004 - val_loss: 1.0029\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0004 - val_loss: 1.0029\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0003 - val_loss: 1.0028\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0003 - val_loss: 1.0028\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0003 - val_loss: 1.0027\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0002 - val_loss: 1.0027\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0002 - val_loss: 1.0027\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0002 - val_loss: 1.0027\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0001 - val_loss: 1.0026\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0001 - val_loss: 1.0026\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0001 - val_loss: 1.0026\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0001 - val_loss: 1.0026\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0001 - val_loss: 1.0025\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0001 - val_loss: 1.0025\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0025\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0025\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0025\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0025\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0024\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0024\n",
      "Epoch 79/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0024\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0024\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 1.0024\n",
      "Epoch 82/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0024\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0024\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0024\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0024\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0024\n",
      "Epoch 87/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0024\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0024\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 92/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 97/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9999 - val_loss: 1.0023\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9998 - val_loss: 1.0023\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9998 - val_loss: 1.0023\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9998 - val_loss: 1.0023\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.209093</td>\n",
       "      <td>-0.312635</td>\n",
       "      <td>1.214318</td>\n",
       "      <td>0.555501</td>\n",
       "      <td>-0.042558</td>\n",
       "      <td>1.648178</td>\n",
       "      <td>0.435323</td>\n",
       "      <td>0.647107</td>\n",
       "      <td>0.037273</td>\n",
       "      <td>1.798863</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237660</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>1.600722</td>\n",
       "      <td>1.329541</td>\n",
       "      <td>-0.525543</td>\n",
       "      <td>-0.076821</td>\n",
       "      <td>1.377121</td>\n",
       "      <td>1.323742</td>\n",
       "      <td>0.876080</td>\n",
       "      <td>11.483888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.028260</td>\n",
       "      <td>0.007307</td>\n",
       "      <td>-0.028382</td>\n",
       "      <td>-0.012984</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>-0.038523</td>\n",
       "      <td>-0.010175</td>\n",
       "      <td>-0.015125</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.042045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028928</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>-0.037414</td>\n",
       "      <td>-0.031075</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>-0.032188</td>\n",
       "      <td>-0.030940</td>\n",
       "      <td>-0.020477</td>\n",
       "      <td>5.330847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        1.209093 -0.312635  1.214318  0.555501 -0.042558  1.648178  0.435323   \n",
       "1       -0.028260  0.007307 -0.028382 -0.012984  0.000995 -0.038523 -0.010175   \n",
       "\n",
       "                7         8         9  ...        24        25        26  \\\n",
       "cluster                                ...                                 \n",
       "0        0.647107  0.037273  1.798863  ... -1.237660  0.035183  1.600722   \n",
       "1       -0.015125 -0.000871 -0.042045  ...  0.028928 -0.000822 -0.037414   \n",
       "\n",
       "               27        28        29        30        31        32      score  \n",
       "cluster                                                                         \n",
       "0        1.329541 -0.525543 -0.076821  1.377121  1.323742  0.876080  11.483888  \n",
       "1       -0.031075  0.012284  0.001796 -0.032188 -0.030940 -0.020477   5.330847  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = AutoEncoder(hidden_neurons =[33, 2, 2, 33])\n",
    "clf1.fit(xtrain)\n",
    "y_train_scores = clf1.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf1.predict(xtest)  # outlier labels (0 or 1)\n",
    "# clf.decision_function: Predict raw anomaly score of X using the fitted detector.\n",
    "y_test_scores = clf1.decision_function(xtest)  # outlier scores\n",
    "\n",
    "y_test_pred = pd.Series(y_test_pred)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "y_train_scores = clf1.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf1.predict(xtest)  # outlier labels (0 or 1)\n",
    "# clf.decision_function: Predict raw anomaly score of X using the fitted detector.\n",
    "y_test_scores = clf1.decision_function(xtest)  # outlier scores\n",
    "\n",
    "y_test_pred = pd.Series(y_test_pred)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_test_scores, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram for Model Clf1 Anomaly Scores\")\n",
    "plt.show()\n",
    "df_test = pd.DataFrame(xtest)\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']>10, 0, 1)\n",
    "df_test['cluster'].value_counts()\n",
    "df_test.groupby('cluster').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ac2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                544       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 33)                561       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 33)                1122      \n",
      "=================================================================\n",
      "Total params: 5,675\n",
      "Trainable params: 5,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 2.8699 - val_loss: 1.6349\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.3822 - val_loss: 1.1859\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.1527 - val_loss: 1.0958\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.1005 - val_loss: 1.0672\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0797 - val_loss: 1.0524\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0674 - val_loss: 1.0425\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0585 - val_loss: 1.0352\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0519 - val_loss: 1.0294\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0464 - val_loss: 1.0245\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0415 - val_loss: 1.0205\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0378 - val_loss: 1.0170\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0344 - val_loss: 1.0142\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0315 - val_loss: 1.0114\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0288 - val_loss: 1.0091\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0265 - val_loss: 1.0070\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0245 - val_loss: 1.0051\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0224 - val_loss: 1.0035\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0208 - val_loss: 1.0019\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0192 - val_loss: 1.0006\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0179 - val_loss: 0.9993\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0165 - val_loss: 0.9981\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0153 - val_loss: 0.9970\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0142 - val_loss: 0.9961\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0132 - val_loss: 0.9952\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0123 - val_loss: 0.9943\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0114 - val_loss: 0.9935\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0106 - val_loss: 0.9929\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0099 - val_loss: 0.9922\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0092 - val_loss: 0.9916\n",
      "Epoch 30/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0086 - val_loss: 0.9911\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0081 - val_loss: 0.9906\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0075 - val_loss: 0.9901\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0071 - val_loss: 0.9897\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0066 - val_loss: 0.9893\n",
      "Epoch 35/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0062 - val_loss: 0.9889\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0059 - val_loss: 0.9886\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0055 - val_loss: 0.9883\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0052 - val_loss: 0.9880\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0049 - val_loss: 0.9878\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0047 - val_loss: 0.9875\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0044 - val_loss: 0.9873\n",
      "Epoch 42/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0042 - val_loss: 0.9871\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0040 - val_loss: 0.9870\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0039 - val_loss: 0.9868\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0037 - val_loss: 0.9866\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0036 - val_loss: 0.9865\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0034 - val_loss: 0.9864\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0033 - val_loss: 0.9863\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0032 - val_loss: 0.9862\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0031 - val_loss: 0.9861\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0030 - val_loss: 0.9860\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0029 - val_loss: 0.9859\n",
      "Epoch 53/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0028 - val_loss: 0.9858\n",
      "Epoch 54/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0027 - val_loss: 0.9858\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0026 - val_loss: 0.9857\n",
      "Epoch 56/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0026 - val_loss: 0.9856\n",
      "Epoch 57/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0025 - val_loss: 0.9856\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0025 - val_loss: 0.9855\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0024 - val_loss: 0.9855\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0024 - val_loss: 0.9855\n",
      "Epoch 61/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0023 - val_loss: 0.9854\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0023 - val_loss: 0.9854\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0023 - val_loss: 0.9853\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0022 - val_loss: 0.9853\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0022 - val_loss: 0.9853\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0022 - val_loss: 0.9853\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0021 - val_loss: 0.9852\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0021 - val_loss: 0.9852\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0021 - val_loss: 0.9852\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0021 - val_loss: 0.9852\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0020 - val_loss: 0.9852\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0020 - val_loss: 0.9851\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0020 - val_loss: 0.9851\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0020 - val_loss: 0.9851\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0020 - val_loss: 0.9851\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0020 - val_loss: 0.9851\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0020 - val_loss: 0.9851\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9851\n",
      "Epoch 79/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0019 - val_loss: 0.9851\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9850\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9850\n",
      "Epoch 82/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9850\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9850\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9850\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9850\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 0.9850\n",
      "Epoch 87/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 92/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0018 - val_loss: 0.9850\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9849\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9849\n",
      "Epoch 97/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9849\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0018 - val_loss: 0.9849\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0018 - val_loss: 0.9849\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0018 - val_loss: 0.9849\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.209093</td>\n",
       "      <td>-0.312635</td>\n",
       "      <td>1.214318</td>\n",
       "      <td>0.555501</td>\n",
       "      <td>-0.042558</td>\n",
       "      <td>1.648178</td>\n",
       "      <td>0.435323</td>\n",
       "      <td>0.647107</td>\n",
       "      <td>0.037273</td>\n",
       "      <td>1.798863</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237660</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>1.600722</td>\n",
       "      <td>1.329541</td>\n",
       "      <td>-0.525543</td>\n",
       "      <td>-0.076821</td>\n",
       "      <td>1.377121</td>\n",
       "      <td>1.323742</td>\n",
       "      <td>0.876080</td>\n",
       "      <td>11.483809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.028260</td>\n",
       "      <td>0.007307</td>\n",
       "      <td>-0.028382</td>\n",
       "      <td>-0.012984</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>-0.038523</td>\n",
       "      <td>-0.010175</td>\n",
       "      <td>-0.015125</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.042045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028928</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>-0.037414</td>\n",
       "      <td>-0.031075</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>-0.032188</td>\n",
       "      <td>-0.030940</td>\n",
       "      <td>-0.020477</td>\n",
       "      <td>5.330853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        1.209093 -0.312635  1.214318  0.555501 -0.042558  1.648178  0.435323   \n",
       "1       -0.028260  0.007307 -0.028382 -0.012984  0.000995 -0.038523 -0.010175   \n",
       "\n",
       "                7         8         9  ...        24        25        26  \\\n",
       "cluster                                ...                                 \n",
       "0        0.647107  0.037273  1.798863  ... -1.237660  0.035183  1.600722   \n",
       "1       -0.015125 -0.000871 -0.042045  ...  0.028928 -0.000822 -0.037414   \n",
       "\n",
       "               27        28        29        30        31        32      score  \n",
       "cluster                                                                         \n",
       "0        1.329541 -0.525543 -0.076821  1.377121  1.323742  0.876080  11.483809  \n",
       "1       -0.031075  0.012284  0.001796 -0.032188 -0.030940 -0.020477   5.330853  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = AutoEncoder(hidden_neurons =[33, 16, 2, 16, 33])\n",
    "clf2.fit(xtrain)\n",
    "y_train_scores = clf2.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf2.predict(xtest)  # outlier labels (0 or 1)\n",
    "# clf.decision_function: Predict raw anomaly score of X using the fitted detector.\n",
    "y_test_scores = clf2.decision_function(xtest)  # outlier scores\n",
    "\n",
    "y_test_pred = pd.Series(y_test_pred)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "y_train_scores = clf2.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf2.predict(xtest)  # outlier labels (0 or 1)\n",
    "# clf.decision_function: Predict raw anomaly score of X using the fitted detector.\n",
    "y_test_scores = clf2.decision_function(xtest)  # outlier scores\n",
    "\n",
    "y_test_pred = pd.Series(y_test_pred)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_test_scores, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram for Model Clf2 Anomaly Scores\")\n",
    "plt.show()\n",
    "df_test = pd.DataFrame(xtest)\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']>10, 0, 1)\n",
    "df_test['cluster'].value_counts()\n",
    "df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ac450e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                544       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 33)                561       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 33)                1122      \n",
      "=================================================================\n",
      "Total params: 5,915\n",
      "Trainable params: 5,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 2.4367 - val_loss: 1.5136\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.2827 - val_loss: 1.1875\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.1291 - val_loss: 1.1263\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0916 - val_loss: 1.1034\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0740 - val_loss: 1.0898\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0624 - val_loss: 1.0803\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0536 - val_loss: 1.0730\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0468 - val_loss: 1.0672\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0415 - val_loss: 1.0624\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0368 - val_loss: 1.0584\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0329 - val_loss: 1.0550\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0296 - val_loss: 1.0519\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0266 - val_loss: 1.0493\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0240 - val_loss: 1.0470\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0217 - val_loss: 1.0450\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0195 - val_loss: 1.0430\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0176 - val_loss: 1.0413\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0159 - val_loss: 1.0397\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0143 - val_loss: 1.0384\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0129 - val_loss: 1.0371\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0115 - val_loss: 1.0360\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0104 - val_loss: 1.0349\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0093 - val_loss: 1.0339\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0084 - val_loss: 1.0330\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0074 - val_loss: 1.0322\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0066 - val_loss: 1.0315\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0057 - val_loss: 1.0307\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0051 - val_loss: 1.0301\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0044 - val_loss: 1.0297\n",
      "Epoch 30/100\n",
      "252/252 [==============================] - ETA: 0s - loss: 1.006 - 1s 2ms/step - loss: 1.0039 - val_loss: 1.0290\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0033 - val_loss: 1.0285\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0028 - val_loss: 1.0281\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0024 - val_loss: 1.0277\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.0019 - val_loss: 1.0273\n",
      "Epoch 35/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0015 - val_loss: 1.0270\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0012 - val_loss: 1.0267\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0009 - val_loss: 1.0264\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0006 - val_loss: 1.0262\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0004 - val_loss: 1.0259\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 1.0001 - val_loss: 1.0257\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9999 - val_loss: 1.0255\n",
      "Epoch 42/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9997 - val_loss: 1.0253\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9995 - val_loss: 1.0252\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9994 - val_loss: 1.0250\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9992 - val_loss: 1.0249\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9991 - val_loss: 1.0248\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9989 - val_loss: 1.0247\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9988 - val_loss: 1.0246\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9987 - val_loss: 1.0245\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9986 - val_loss: 1.0244\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9985 - val_loss: 1.0243\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9985 - val_loss: 1.0243\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9984 - val_loss: 1.0242\n",
      "Epoch 54/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9983 - val_loss: 1.0242\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9983 - val_loss: 1.0241\n",
      "Epoch 56/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9982 - val_loss: 1.0241\n",
      "Epoch 57/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9981 - val_loss: 1.0240\n",
      "Epoch 58/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9981 - val_loss: 1.0240\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9980 - val_loss: 1.0239\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9980 - val_loss: 1.0239\n",
      "Epoch 61/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9980 - val_loss: 1.0239\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9979 - val_loss: 1.0238\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9979 - val_loss: 1.0238\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9979 - val_loss: 1.0238\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9978 - val_loss: 1.0238\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9978 - val_loss: 1.0237\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9978 - val_loss: 1.0237\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9978 - val_loss: 1.0237\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9977 - val_loss: 1.0237\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9977 - val_loss: 1.0237\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9977 - val_loss: 1.0237\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9977 - val_loss: 1.0237\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9977 - val_loss: 1.0236\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9977 - val_loss: 1.0236\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9977 - val_loss: 1.0236\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 79/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 82/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9976 - val_loss: 1.0236\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 1.0235\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9976 - val_loss: 1.0235\n",
      "Epoch 87/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 92/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 1s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 97/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 0.9975 - val_loss: 1.0235\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.209093</td>\n",
       "      <td>-0.312635</td>\n",
       "      <td>1.214318</td>\n",
       "      <td>0.555501</td>\n",
       "      <td>-0.042558</td>\n",
       "      <td>1.648178</td>\n",
       "      <td>0.435323</td>\n",
       "      <td>0.647107</td>\n",
       "      <td>0.037273</td>\n",
       "      <td>1.798863</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237660</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>1.600722</td>\n",
       "      <td>1.329541</td>\n",
       "      <td>-0.525543</td>\n",
       "      <td>-0.076821</td>\n",
       "      <td>1.377121</td>\n",
       "      <td>1.323742</td>\n",
       "      <td>0.876080</td>\n",
       "      <td>11.483979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.028260</td>\n",
       "      <td>0.007307</td>\n",
       "      <td>-0.028382</td>\n",
       "      <td>-0.012984</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>-0.038523</td>\n",
       "      <td>-0.010175</td>\n",
       "      <td>-0.015125</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.042045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028928</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>-0.037414</td>\n",
       "      <td>-0.031075</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>-0.032188</td>\n",
       "      <td>-0.030940</td>\n",
       "      <td>-0.020477</td>\n",
       "      <td>5.330830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        1.209093 -0.312635  1.214318  0.555501 -0.042558  1.648178  0.435323   \n",
       "1       -0.028260  0.007307 -0.028382 -0.012984  0.000995 -0.038523 -0.010175   \n",
       "\n",
       "                7         8         9  ...        24        25        26  \\\n",
       "cluster                                ...                                 \n",
       "0        0.647107  0.037273  1.798863  ... -1.237660  0.035183  1.600722   \n",
       "1       -0.015125 -0.000871 -0.042045  ...  0.028928 -0.000822 -0.037414   \n",
       "\n",
       "               27        28        29        30        31        32      score  \n",
       "cluster                                                                         \n",
       "0        1.329541 -0.525543 -0.076821  1.377121  1.323742  0.876080  11.483979  \n",
       "1       -0.031075  0.012284  0.001796 -0.032188 -0.030940 -0.020477   5.330830  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = AutoEncoder(hidden_neurons =[33, 16, 8, 2, 8, 16, 33])\n",
    "clf3.fit(xtrain)\n",
    "y_train_scores = clf3.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf3.predict(xtest)  # outlier labels (0 or 1)\n",
    "# clf.decision_function: Predict raw anomaly score of X using the fitted detector.\n",
    "y_test_scores = clf3.decision_function(xtest)  # outlier scores\n",
    "\n",
    "y_test_pred = pd.Series(y_test_pred)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "y_train_scores = clf3.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf3.predict(xtest)  # outlier labels (0 or 1)\n",
    "# clf.decision_function: Predict raw anomaly score of X using the fitted detector.\n",
    "y_test_scores = clf3.decision_function(xtest)  # outlier scores\n",
    "\n",
    "y_test_pred = pd.Series(y_test_pred)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_test_scores, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram for Model Clf3 Anomaly Scores\")\n",
    "plt.show()\n",
    "df_test = pd.DataFrame(xtest)\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']>10, 0, 1)\n",
    "df_test['cluster'].value_counts()\n",
    "df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "815c00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "\n",
    "# Put all the predictions in a data frame\n",
    "train_scores = pd.DataFrame({'clf1': clf1.decision_scores_,\n",
    "                             'clf2': clf2.decision_scores_,\n",
    "                             'clf3': clf3.decision_scores_\n",
    "                            })\n",
    "\n",
    "test_scores  = pd.DataFrame({'clf1': clf1.decision_function(xtest),\n",
    "                             'clf2': clf2.decision_function(xtest),\n",
    "                             'clf3': clf3.decision_function(xtest) \n",
    "                            })\n",
    "\n",
    "from pyod.utils.utility import standardizer\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,test_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db0c41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>y_by_average_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.143231</td>\n",
       "      <td>-0.722856</td>\n",
       "      <td>1.388728</td>\n",
       "      <td>0.553246</td>\n",
       "      <td>-0.093456</td>\n",
       "      <td>1.324666</td>\n",
       "      <td>0.411110</td>\n",
       "      <td>0.592688</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>2.013692</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.257266</td>\n",
       "      <td>0.465280</td>\n",
       "      <td>1.321144</td>\n",
       "      <td>0.697031</td>\n",
       "      <td>-0.999653</td>\n",
       "      <td>0.292846</td>\n",
       "      <td>1.450986</td>\n",
       "      <td>1.761443</td>\n",
       "      <td>1.462686</td>\n",
       "      <td>3.808228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.017676</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>-0.021472</td>\n",
       "      <td>-0.008554</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>-0.006356</td>\n",
       "      <td>-0.009164</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019439</td>\n",
       "      <td>-0.007194</td>\n",
       "      <td>-0.020427</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>0.015456</td>\n",
       "      <td>-0.004528</td>\n",
       "      <td>-0.022435</td>\n",
       "      <td>-0.027235</td>\n",
       "      <td>-0.022615</td>\n",
       "      <td>-0.062258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2         3         4  \\\n",
       "y_by_average_cluster                                                     \n",
       "0                     1.143231 -0.722856  1.388728  0.553246 -0.093456   \n",
       "1                    -0.017676  0.011176 -0.021472 -0.008554  0.001445   \n",
       "\n",
       "                             5         6         7         8         9  ...  \\\n",
       "y_by_average_cluster                                                    ...   \n",
       "0                     1.324666  0.411110  0.592688  0.009567  2.013692  ...   \n",
       "1                    -0.020481 -0.006356 -0.009164 -0.000148 -0.031135  ...   \n",
       "\n",
       "                            24        25        26        27        28  \\\n",
       "y_by_average_cluster                                                     \n",
       "0                    -1.257266  0.465280  1.321144  0.697031 -0.999653   \n",
       "1                     0.019439 -0.007194 -0.020427 -0.010777  0.015456   \n",
       "\n",
       "                            29        30        31        32  \\\n",
       "y_by_average_cluster                                           \n",
       "0                     0.292846  1.450986  1.761443  1.462686   \n",
       "1                    -0.004528 -0.022435 -0.027235 -0.022615   \n",
       "\n",
       "                      y_by_average_score  \n",
       "y_by_average_cluster                      \n",
       "0                               3.808228  \n",
       "1                              -0.062258  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_by_average = average(test_scores_norm)\n",
    "             \n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_by_average, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(xtest)\n",
    "df_test['y_by_average_score'] = y_by_average\n",
    "df_test['y_by_average_cluster'] = np.where(df_test['y_by_average_score']>3, 0, 1)\n",
    "df_test['y_by_average_cluster'].value_counts()\n",
    "df_test.groupby('y_by_average_cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d133030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>y_by_maximization_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_maximization_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.143231</td>\n",
       "      <td>-0.722856</td>\n",
       "      <td>1.388728</td>\n",
       "      <td>0.553246</td>\n",
       "      <td>-0.093456</td>\n",
       "      <td>1.324666</td>\n",
       "      <td>0.411110</td>\n",
       "      <td>0.592688</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>2.013692</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.257266</td>\n",
       "      <td>0.465280</td>\n",
       "      <td>1.321144</td>\n",
       "      <td>0.697031</td>\n",
       "      <td>-0.999653</td>\n",
       "      <td>0.292846</td>\n",
       "      <td>1.450986</td>\n",
       "      <td>1.761443</td>\n",
       "      <td>1.462686</td>\n",
       "      <td>3.808460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.017676</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>-0.021472</td>\n",
       "      <td>-0.008554</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>-0.006356</td>\n",
       "      <td>-0.009164</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019439</td>\n",
       "      <td>-0.007194</td>\n",
       "      <td>-0.020427</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>0.015456</td>\n",
       "      <td>-0.004528</td>\n",
       "      <td>-0.022435</td>\n",
       "      <td>-0.027235</td>\n",
       "      <td>-0.022615</td>\n",
       "      <td>-0.062031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0         1         2         3         4  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                          1.143231 -0.722856  1.388728  0.553246 -0.093456   \n",
       "1                         -0.017676  0.011176 -0.021472 -0.008554  0.001445   \n",
       "\n",
       "                                  5         6         7         8         9  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                          1.324666  0.411110  0.592688  0.009567  2.013692   \n",
       "1                         -0.020481 -0.006356 -0.009164 -0.000148 -0.031135   \n",
       "\n",
       "                           ...        24        25        26        27  \\\n",
       "y_by_maximization_cluster  ...                                           \n",
       "0                          ... -1.257266  0.465280  1.321144  0.697031   \n",
       "1                          ...  0.019439 -0.007194 -0.020427 -0.010777   \n",
       "\n",
       "                                 28        29        30        31        32  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                         -0.999653  0.292846  1.450986  1.761443  1.462686   \n",
       "1                          0.015456 -0.004528 -0.022435 -0.027235 -0.022615   \n",
       "\n",
       "                           y_by_maximization_score  \n",
       "y_by_maximization_cluster                           \n",
       "0                                         3.808460  \n",
       "1                                        -0.062031  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combination by max\n",
    "y_by_maximization = maximization(test_scores_norm)\n",
    "             \n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_by_maximization, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by max\")\n",
    "plt.show()\n",
    "\n",
    "df_test = pd.DataFrame(xtest)\n",
    "df_test['y_by_maximization_score'] = y_by_maximization\n",
    "df_test['y_by_maximization_cluster'] = np.where(df_test['y_by_maximization_score']>3, 0, 1)\n",
    "df_test['y_by_maximization_cluster'].value_counts()\n",
    "df_test.groupby('y_by_maximization_cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
